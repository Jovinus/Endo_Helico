{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:24:07.039533Z",
     "start_time": "2021-03-26T06:24:04.637837Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU가 있으면 사용, 아닐경우 CPU사용 (GPU사용을 추천합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:24:08.277763Z",
     "start_time": "2021-03-26T06:24:08.185961Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 읽어오기 및 전처리\n",
    "result_text가 원본 text이며,  text_processed가 전처리된 text입니다.\n",
    "\n",
    "result_text를 text_preprocessed로 바꾸는 과정을 \"텍스트 전처리\" 라고 부르겠습니다.\n",
    "텍스트 전처리를 수행한이유는 Bert 전처리에서 설명되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:21:09.966988Z",
     "start_time": "2021-03-26T07:21:07.215475Z"
    }
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv('./helico_preprocessed.csv')\n",
    "csv = csv.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:21:09.980710Z",
     "start_time": "2021-03-26T07:21:09.969156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SM_DATE</th>\n",
       "      <th>EXEC_TIME</th>\n",
       "      <th>처방코드</th>\n",
       "      <th>result_text</th>\n",
       "      <th>h_pyl_positive</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A020CFDF2D</td>\n",
       "      <td>2019-04-11 00:00:00</td>\n",
       "      <td>2019-04-11 13:46:56</td>\n",
       "      <td>BP1A151A</td>\n",
       "      <td>▣ 결론 및 진단\\r\\n\\r\\n(507011)\\r\\n\\r\\n             ...</td>\n",
       "      <td>1</td>\n",
       "      <td>507011 THE UPDATED SYDNEY SYSTEM Site H. pylo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00A0347B006B</td>\n",
       "      <td>2020-08-07 00:00:00</td>\n",
       "      <td>2020-08-07 15:51:42</td>\n",
       "      <td>BP1A151A</td>\n",
       "      <td>▣ 결론 및 진단\\r\\n\\r\\n(219350)\\r\\n\\r\\n             ...</td>\n",
       "      <td>0</td>\n",
       "      <td>219350 THE UPDATED SYDNEY SYSTEM Site H. pylo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00A091037DE1</td>\n",
       "      <td>2019-01-25 00:00:00</td>\n",
       "      <td>2019-01-25 14:36:40</td>\n",
       "      <td>BP1A151A</td>\n",
       "      <td>▣ 결론 및 진단\\r\\n\\r\\n(507011)\\r\\n\\r\\n1. Stomach, #...</td>\n",
       "      <td>0</td>\n",
       "      <td>507011 1. Stomach, #1x1 : GC of low body, bio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00A0A50F463F</td>\n",
       "      <td>2018-12-21 00:00:00</td>\n",
       "      <td>2018-12-21 15:59:49</td>\n",
       "      <td>BP1A151A</td>\n",
       "      <td>▣ 결론 및 진단\\r\\n\\r\\n(PP4970)\\r\\n\\r\\n  Stomach, #1...</td>\n",
       "      <td>0</td>\n",
       "      <td>PP4970 Stomach, #1x1 : Anterior wall of low b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00A0C41326F7</td>\n",
       "      <td>2019-05-17 00:00:00</td>\n",
       "      <td>2019-05-17 15:46:34</td>\n",
       "      <td>BP1A151A</td>\n",
       "      <td>▣ 결론 및 진단\\r\\n\\r\\n(298059)\\r\\n\\r\\n  Stomach, #1...</td>\n",
       "      <td>0</td>\n",
       "      <td>298059 Stomach, #1x1 : fundus, biopsy : . Fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID              SM_DATE            EXEC_TIME      처방코드  \\\n",
       "0  00A020CFDF2D  2019-04-11 00:00:00  2019-04-11 13:46:56  BP1A151A   \n",
       "1  00A0347B006B  2020-08-07 00:00:00  2020-08-07 15:51:42  BP1A151A   \n",
       "2  00A091037DE1  2019-01-25 00:00:00  2019-01-25 14:36:40  BP1A151A   \n",
       "3  00A0A50F463F  2018-12-21 00:00:00  2018-12-21 15:59:49  BP1A151A   \n",
       "4  00A0C41326F7  2019-05-17 00:00:00  2019-05-17 15:46:34  BP1A151A   \n",
       "\n",
       "                                         result_text  h_pyl_positive  \\\n",
       "0  ▣ 결론 및 진단\\r\\n\\r\\n(507011)\\r\\n\\r\\n             ...               1   \n",
       "1  ▣ 결론 및 진단\\r\\n\\r\\n(219350)\\r\\n\\r\\n             ...               0   \n",
       "2  ▣ 결론 및 진단\\r\\n\\r\\n(507011)\\r\\n\\r\\n1. Stomach, #...               0   \n",
       "3  ▣ 결론 및 진단\\r\\n\\r\\n(PP4970)\\r\\n\\r\\n  Stomach, #1...               0   \n",
       "4  ▣ 결론 및 진단\\r\\n\\r\\n(298059)\\r\\n\\r\\n  Stomach, #1...               0   \n",
       "\n",
       "                                      text_processed  \n",
       "0   507011 THE UPDATED SYDNEY SYSTEM Site H. pylo...  \n",
       "1   219350 THE UPDATED SYDNEY SYSTEM Site H. pylo...  \n",
       "2   507011 1. Stomach, #1x1 : GC of low body, bio...  \n",
       "3   PP4970 Stomach, #1x1 : Anterior wall of low b...  \n",
       "4   298059 Stomach, #1x1 : fundus, biopsy : . Fun...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:21:13.087940Z",
     "start_time": "2021-03-26T07:21:13.074801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▣ 결론 및 진단\r\n",
      "\r\n",
      "(507011)\r\n",
      "\r\n",
      "                              THE UPDATED SYDNEY SYSTEM\r\n",
      "====================================================================================\r\n",
      "Site            H. pylori       Neutrophil    Mononuclear   Atrophy     Intestinal\r\n",
      "                colonization     activity          cells                 metaplasia\r\n",
      "------------------------------------------------------------------------------------\r\n",
      "1.#1x1 : antrum    mild         mild           moderate      not applicable  absent    \r\n",
      "------------------------------------------------------------------------------------\r\n",
      "2.#2x1 : antrum    mild         mild           moderate      not applicable  absent\r\n",
      "====================================================================================\r\n",
      "\r\n",
      "1. Stomach, #1x1 : LC of distal antrum, biopsy :\r\n",
      "\r\n",
      "  . H. pylori-chronic gastritis, active, with erosion\r\n",
      "\r\n",
      "2. Stomach, #2x1 : LC of prepyloric antrum, biopsy :\r\n",
      "\r\n",
      "  . H. pylori-chronic gastritis, active, with erosion\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(csv['result_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:21:14.492484Z",
     "start_time": "2021-03-26T07:21:14.487911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 507011 THE UPDATED SYDNEY SYSTEM Site H. pylori Neutrophil Mononuclear Atrophy Intestinal colonization activity cells metaplasia 1.#1x1 : antrum mild mild moderate not applicable absent 2.#2x1 : antrum mild mild moderate not applicable absent 1. Stomach, #1x1 : LC of distal antrum, biopsy : . H. pylori chronic gastritis, active, with erosion 2. Stomach, #2x1 : LC of prepyloric antrum, biopsy : . H. pylori chronic gastritis, active, with erosion \n"
     ]
    }
   ],
   "source": [
    "print(csv['text_processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:19:21.984368Z",
     "start_time": "2021-03-26T07:19:21.856071Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(csv, test_size=0.2, random_state=1004, stratify=csv['h_pyl_positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert 전처리 모델 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:28:22.023252Z",
     "start_time": "2021-03-26T06:28:18.823489Z"
    },
    "code_folding": [
     2,
     71
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/google/experts/bert/wiki_books/2\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'experts_wiki_books' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리의 이유\n",
    "## 텍스트 전처리 이전\n",
    "\n",
    "THE UPDATED SYDNEY SYSTEM를 삭제한 이유는 다음과같습니다. 텍스트 전처리 이전의 text인 'result_text'를 넣었을때에는 bert preprocess model의 output에서 input_word_ids에 1027이라는 숫자가 많이보입니다. 즉 의미없는 데이터(분류하는데 크게 기여하지않으리라고 예상이되는)가 많이 존재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:19:25.614548Z",
     "start_time": "2021-03-26T07:19:25.571078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,   100,  1455, 30010, 30022, 29994, 30011, 30021,   100,\n",
       "          1464, 30019, 30021, 29993, 30006, 30021,  1006,  2753, 19841,\n",
       "         14526,  1007,  1996,  7172,  3994,  2291,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          2609,  1044,  1012,  1052,  8516, 10050, 11265,  4904, 18981,\n",
       "         19466, 18847, 11231, 14321,  2906,  2012, 18981, 10536, 20014,\n",
       "         19126,   102]], dtype=int32)>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_preprocess_model([train['result_text'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:19:26.400081Z",
     "start_time": "2021-03-26T07:19:26.386647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,   100,  1455, 30010, 30022, 29994, 30011, 30021,   100,\n",
       "          1464, 30019, 30021, 29993, 30006, 30021,  1006, 20636, 19481,\n",
       "          2692,  1007,  1996,  7172,  3994,  2291,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,  1027,\n",
       "          2609,  1044,  1012,  1052,  8516, 10050, 11265,  4904, 18981,\n",
       "         19466, 18847, 11231, 14321,  2906,  2012, 18981, 10536, 20014,\n",
       "         19126,   102]], dtype=int32)>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_preprocess_model([train['result_text'][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 후\n",
    "반대로, THE UPDATED SYDNEY SYSTEM를 제거한 이후 bert_preprocess_model에 넣어준 결과는 다음과같습니다. 텍스트 전처리 이전과는 다르게 많은부분을 차지하고있던 1027이 삭제된것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:19:29.420011Z",
     "start_time": "2021-03-26T07:19:29.407738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101,  2753, 19841, 14526,  1996,  7172,  3994,  2291,  2609,\n",
       "          1044,  1012,  1052,  8516, 10050, 11265,  4904, 18981, 19466,\n",
       "         18847, 11231, 14321,  2906,  2012, 18981, 10536, 20014, 19126,\n",
       "         18962,  4023,  4442, 18804, 24759, 15396,  1015,  1012,  1001,\n",
       "          1015,  2595,  2487,  1024, 14405,  6824, 10256, 10256,  8777,\n",
       "          2025, 12711,  9962,  1016,  1012,  1001,  1016,  2595,  2487,\n",
       "          1024, 14405,  6824, 10256, 10256,  8777,  2025, 12711,  9962,\n",
       "          1015,  1012,  4308,  1010,  1001,  1015,  2595,  2487,  1024,\n",
       "         29215,  1997, 29333, 14405,  6824,  1010, 16012, 18075,  1024,\n",
       "          1012,  1044,  1012,  1052,  8516, 10050, 11888,  3806, 18886,\n",
       "          7315,  1010,  3161,  1010,  2007, 14173,  1016,  1012,  4308,\n",
       "          1010,  1001,  1016,  2595,  2487,  1024, 29215,  1997, 17463,\n",
       "          8516, 29180, 14405,  6824,  1010, 16012, 18075,  1024,  1012,\n",
       "          1044,  1012,  1052,  8516, 10050, 11888,  3806, 18886,  7315,\n",
       "          1010,   102]], dtype=int32)>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_preprocess_model([train['text_processed'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:19:30.078451Z",
     "start_time": "2021-03-26T07:19:30.065257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       dtype=int32)>,\n",
       " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[  101, 20636, 19481,  2692,  1996,  7172,  3994,  2291,  2609,\n",
       "          1044,  1012,  1052,  8516, 10050, 11265,  4904, 18981, 19466,\n",
       "         18847, 11231, 14321,  2906,  2012, 18981, 10536, 20014, 19126,\n",
       "         18962,  4023,  4442, 18804, 24759, 15396,  1015,  1012,  1001,\n",
       "          1015,  2595,  2487,  1024, 14405,  6824,  9962,  9962, 10256,\n",
       "          2025, 12711,  4417,  1016,  1012,  1001,  1016,  2595,  2487,\n",
       "          1024, 14405,  6824,  9962,  9962, 10256,  2025, 12711,  4417,\n",
       "          1015,  1012,  4308,  1010,  1001,  1015,  2595,  2487,  1024,\n",
       "         15219,  2813,  1997, 29333, 14405,  6824,  1010, 16012, 18075,\n",
       "          1024,  1012, 11888,  3806, 18886,  7315,  1010, 16389,  1010,\n",
       "          2007, 20014, 19126, 18804, 24759, 15396, 12958,  2828,  1998,\n",
       "         14173,  1012,  2053,  1044,  1012,  1052,  8516, 10050,  4453,\n",
       "          1012,  1016,  1012,  4308,  1010,  1001,  1016,  2595,  2487,\n",
       "          1024, 15219,  2813,  1997,  3054, 14405,  6824,  1010, 16012,\n",
       "         18075,   102]], dtype=int32)>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_preprocess_model([train['text_processed'][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 build 및 학습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:29:15.490717Z",
     "start_time": "2021-03-26T06:29:15.482287Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:29:35.051233Z",
     "start_time": "2021-03-26T06:29:23.658129Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "# bert_raw_result = classifier_model(tf.constant(test_text))\n",
    "# print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "# metrics = [tf.metrics.BinaryAccuracy(), tf.metrics.AUC(), tf.metrics.Recall()]\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "steps_per_epoch = len(train) // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 후의 데이터를 사용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T07:18:20.339321Z",
     "start_time": "2021-03-26T06:29:39.688032Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3912/3912 [==============================] - 1981s 502ms/step - loss: 0.1428 - accuracy: 0.9275\n",
      "Epoch 2/2\n",
      "1857/3912 [=============>................] - ETA: 17:19 - loss: 0.0151 - accuracy: 0.9972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c203f99782a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = classifier_model.fit(train['text_processed'], train['h_pyl_positive'],\n\u001b[1;32m      2\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                epochs=2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/nam/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(train['text_processed'], train['h_pyl_positive'],\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:29:21.791544Z",
     "start_time": "2021-03-26T06:29:20.060Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_model.evaluate(val['text_processed'], val['h_pyl_positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 이전의 데이터를 사용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:02:45.074108Z",
     "start_time": "2021-03-26T06:02:33.715793Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "# bert_raw_result = classifier_model(tf.constant(test_text))\n",
    "# print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "# metrics = [tf.metrics.BinaryAccuracy(), tf.metrics.AUC(), tf.metrics.Recall()]\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "steps_per_epoch = len(train) // batch_size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T06:08:20.163846Z",
     "start_time": "2021-03-26T06:02:45.076387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "311/311 [==============================] - 176s 506ms/step - loss: 0.6514 - accuracy: 0.6466\n",
      "Epoch 2/2\n",
      "311/311 [==============================] - 159s 512ms/step - loss: 0.6282 - accuracy: 0.6505\n"
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(train['result_text'], train['h_pyl_positive'],\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
